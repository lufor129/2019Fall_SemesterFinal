{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import threading\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value(title):\n",
    "    pipe = [\"維基\",\"Server Error\",\"Access denied\",\"Yahoo is now\",\"Page Not Found\",\"痞客邦\",\"百度\",\"404\",\"Security\"]\n",
    "    if any(x in title for x in pipe):\n",
    "        return True\n",
    "    elif (re.search(\"[^a-zA-Z0-9\\b]+\",title)):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news(l,all_ls):\n",
    "    newslink = hostName+ l\n",
    "    response = requests.get(newslink);\n",
    "    html = etree.HTML(response.content)\n",
    "    origin_news = \"\".join(html.xpath(\"//div[@class='jsx-1919529045 message']/text()\"))\n",
    "    if(len(origin_news)>70 or len(origin_news)<3 or check_value(origin_news)) :\n",
    "        return 0;\n",
    "    new_titles = html.xpath(\"//section[@class='jsx-588669885 links']//article[@class='jsx-1682249194 link']/h1/@title\")\n",
    "    reason = \"\".join(html.xpath(\"//section[@class='jsx-3677418999 jsx-423907629 section']//div/text()\"))\n",
    "    for i in new_titles:\n",
    "        if(check_value(i)):\n",
    "            return 0\n",
    "        dictionary = {}\n",
    "        dictionary[\"link\"] = newslink\n",
    "        dictionary[\"news1\"] = origin_news\n",
    "        dictionary[\"news2\"] = i\n",
    "        dictionary[\"fact\"] = False\n",
    "        dictionary[\"reason\"] = reason\n",
    "        all_ls.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostName = \"https://cofacts.g0v.tw\"\n",
    "response = requests.get(\"https://cofacts.g0v.tw/replies?after=&before=&filter=RUMOR\")\n",
    "html = etree.HTML(response.content)\n",
    "newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "total_number = html.xpath('//p[@class=\"jsx-3107014753\"]/text()')[0]\n",
    "all_ls = []\n",
    "count = 0\n",
    "while(True):\n",
    "    threads = []\n",
    "    for l in newslink_ls:\n",
    "        count += 1\n",
    "        threads.append(threading.Thread(target=clean_news,args=(l,all_ls)))\n",
    "        threads[-1].start()\n",
    "        if(count%100 == 0  and count != 0):\n",
    "            print(count)\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    response = requests.get(\"https://cofacts.g0v.tw/replies?\"+next_link)\n",
    "    html = etree.HTML(response.content)\n",
    "    try:\n",
    "        newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "        next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "    except:\n",
    "        break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = all_ls[0].keys()\n",
    "with open('fake.csv', 'w',encoding=\"utf8\") as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from langconv import *\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dict_path = 'corpora_dict.dict'\n",
    "tfidf_model_path = \"./tfidf.model\"\n",
    "corpus_path = './corpus.mm'\n",
    "corpus = corpora.MmCorpus(corpus_path)\n",
    "tfidf = models.TfidfModel.load(tfidf_model_path)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "dictionary = corpora.Dictionary.load(corpora_dict_path)\n",
    "index = similarities.Similarity(\"./tempName\",corpus_tfidf,len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    r = '[’!\"#$%&\\'()*+,./:;<=>?@，。?★、…【】\\n《》\\s？“”‘’！[\\\\]^_`{|}~「」:]+'\n",
    "    sentence = Converter('zh-hant').convert(sentence)\n",
    "    words = re.sub(r,\"\",sentence)\n",
    "    wordarr = [word for word in jieba.cut(words)]\n",
    "    return \" \".join(wordarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRelateRate(test_doc):\n",
    "    test_doc = [clean_sentence(text) for text in test_doc]\n",
    "    new_vecs = [dictionary.doc2bow(test.split()) for test in test_doc]\n",
    "    new_vec_tfidf_ls = [tfidf[new_vec] for new_vec in new_vecs] \n",
    "    sims = [index[new_vec_tfidf] for new_vec_tfidf in new_vec_tfidf_ls]\n",
    "    sim_index = np.argmax(np.array(sims[0]))\n",
    "    return sims[1][sim_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "with open(\"fake.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0):\n",
    "            continue\n",
    "        if(calRelateRate(row[1:3])):\n",
    "            arr.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5372\n"
     ]
    }
   ],
   "source": [
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake_clean.csv\",\"w\",encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in arr:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import csv\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ''.join([\n",
    "        word for word, flag in words if flag != 'x'])\n",
    "def stringify(text):\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\PC-Henry\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.841 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['看看香港黃之鋒的惡行惡狀欺負老人家算什麼好漢暴徒來台卻變成英雄貴賓把台灣道德價值摧毀殆盡綠色執政崩壞保證', '錯誤網傳影片看看香港黃之鋒的惡行惡狀充分沒有教養的港獨暴力份子...欺負老人家算什麼好漢', 'False'], ['燒烤時戴隱形眼鏡遇高溫會使隱形眼鏡熔化而致盲', '錯誤網傳在烤肉...請將你的隱形眼鏡拿掉一個21歲男生生火時過熱的溫度熔化隱形眼鏡', 'False'], ['剛出爐的麵包傷胃', '謠言終結站剛出爐的麵包會致癌專家斥無稽生活自由時報電子報', 'False'], ['昨晚香港一名參與示威女子被同伴擊中眼睛可能致視力嚴重受損他們竟還反污衊香港警方', '早報中國央視報導香港女子右眼受傷引用假照片新聞信源無可稽考端傳媒InitiumMedia', 'False'], ['你還在吃龍眼嗎看完後震驚了一定要告訴家人啊分享', '假養生近視從800度降到200度謠言讓醫生表示效果有限不要過量了MyGoPen', 'False'], ['你還在吃龍眼嗎看完後震驚了一定要告訴家人啊分享', '龍眼茶降近視謠言說龍眼茶一喝近視度數就狂降這太神奇了我的老天鵝啊', 'False'], ['3C廢材回收換現金', '假圖片3C廢材回收換現金LINE謠言均採用購物抵用金', 'False'], ['直接對口喝易拉罐會致死', '假新聞易開罐飲料對嘴喝會因爲老鼠尿鉤端螺旋體病致死老謠言', 'False'], ['演員張豐毅的孫子被自家養的狗咬出血了得了狂犬病', '假影片被家裡的狗抓到出血小孩發作學狗叫不是狂犬病啦', 'False'], ['嫌犯示範破窗盜竊法一把粗鹽一口口水就可以破窗', '假影片一把粗鹽一口口水就能破窗非鹽巴謠言改編影片', 'False']]\n"
     ]
    }
   ],
   "source": [
    "all_ = []\n",
    "with open(\"./fake_clean2.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0):\n",
    "            continue\n",
    "        all_.append([jieba_tokenizer(row[0]),jieba_tokenizer(row[1]),row[2]])\n",
    "print(all_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['全台83間夜市200元抵用券下周發放擴大秋冬國旅補助第二波領取方式看過來', '200元夜市抵用券明天發放使用限制一次統整聯合新聞網', 'TRUE'], ['酪梨籽殺死癌細胞食藥署根本不能吃', '聽說酪梨籽可以殺死癌細胞這是真的衛生福利部食品藥物管理署', 'TRUE'], ['恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒LINETODAY', '新頭殼恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒', 'TRUE'], ['恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒LINETODAY', '嬌生急收隱形眼鏡這六款批號產品別再戴熱門話題產業經濟日報', 'TRUE'], ['中橫公路中斷了19年即將在10月底開放通行以後遊客前往梨山就不用繞到合歡山走四五個小時只要70分鐘就能抵達', '東森新聞中橫公路中斷了19年即將在10月底開放通行以後遊客前往梨山就不用繞到合歡山走四五個小時只要70分...Facebook', 'TRUE'], ['發票別傻傻印出來恐錯過百萬獎金LINETODAY', '三立新聞網發票別傻傻印出來恐錯過百萬獎金', 'TRUE'], ['微信偷偷綁定臉書帳號快照這步驟刪除是真的沒那麼恐怖啦...', '微信WeChat自動綁定臉書快刪除事情的真相沒那麼恐怖...蘋果仁你的科技媒體', 'TRUE'], ['微信偷偷綁定臉書帳號快照這步驟刪除是真的沒那麼恐怖啦...', '微信WeChat自動綁定臉書快刪除事情的真相沒那麼恐怖...蘋果仁你的科技媒體', 'TRUE'], ['恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒LINETODAY', '新頭殼恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒', 'TRUE'], ['恐影響視力嬌生這款隱眼驚見異物漂浮緊急回收500盒LINETODAY', '嬌生急收隱形眼鏡這六款批號產品別再戴熱門話題產業經濟日報', 'TRUE']]\n"
     ]
    }
   ],
   "source": [
    "all2_= []\n",
    "with open(\"./unfake_clean.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row[0]) == 0):\n",
    "            continue\n",
    "        all2_.append([jieba_tokenizer(row[0]),jieba_tokenizer(row[1]),row[2]])\n",
    "print(all2_[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test re.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"自由時報黑MyGoPen去到哪?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "r = \"自由時報|MyGoPen|LINETODAY|三立新聞網|政經大事|經濟日報|噓星聞|老外瘋台灣|Google搜尋|Yahoo奇摩新聞|雜誌出版推薦|中時電子報|天下雜誌|ETtoday東森新聞雲|LINE台灣\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'黑去到哪?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r,\"\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>看看香港-[黃之鋒]的惡行惡狀欺負老人家算什麼好漢？暴徒來台卻變成英雄貴賓把台灣道德價值摧毀...</td>\n",
       "      <td>【錯誤】網傳影片「看看香港，黃之鋒的惡行惡狀，充分沒有教養的港獨暴力份子，...欺負老人家算...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3、燒烤時戴隱形眼鏡遇高溫會使隱形眼鏡熔化而致盲</td>\n",
       "      <td>【錯誤】網傳「在烤肉...請將你的隱形眼鏡拿掉！一個21歲男生生火時，過熱的溫度熔化隱形眼鏡」？</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剛出爐的麵包傷胃</td>\n",
       "      <td>《謠言終結站》剛出爐的麵包會致癌？ 專家斥無稽 - 生活 - 自由時報電子報</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>昨晚，香港一名參與示威女子被同伴擊中眼睛，可能致視力嚴重受損，他們竟還反污衊香港警方</td>\n",
       "      <td>早報：中國央視報導香港女子右眼受傷，引用假照片，「新聞」信源無可稽考｜端傳媒 Initium...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你還在吃龍眼嗎？看完後震驚了！一定要告訴家人啊！分享~：</td>\n",
       "      <td>【假養生】近視從800度降到200度？謠言讓醫生表示效果有限，不要過量了！ ｜MyGoPen</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title1  \\\n",
       "0  看看香港-[黃之鋒]的惡行惡狀欺負老人家算什麼好漢？暴徒來台卻變成英雄貴賓把台灣道德價值摧毀...   \n",
       "1                           3、燒烤時戴隱形眼鏡遇高溫會使隱形眼鏡熔化而致盲   \n",
       "2                                           剛出爐的麵包傷胃   \n",
       "3         昨晚，香港一名參與示威女子被同伴擊中眼睛，可能致視力嚴重受損，他們竟還反污衊香港警方   \n",
       "4                       你還在吃龍眼嗎？看完後震驚了！一定要告訴家人啊！分享~：   \n",
       "\n",
       "                                              title2  label  \n",
       "0  【錯誤】網傳影片「看看香港，黃之鋒的惡行惡狀，充分沒有教養的港獨暴力份子，...欺負老人家算...  False  \n",
       "1   【錯誤】網傳「在烤肉...請將你的隱形眼鏡拿掉！一個21歲男生生火時，過熱的溫度熔化隱形眼鏡」？  False  \n",
       "2             《謠言終結站》剛出爐的麵包會致癌？ 專家斥無稽 - 生活 - 自由時報電子報  False  \n",
       "3  早報：中國央視報導香港女子右眼受傷，引用假照片，「新聞」信源無可稽考｜端傳媒 Initium...  False  \n",
       "4     【假養生】近視從800度降到200度？謠言讓醫生表示效果有限，不要過量了！ ｜MyGoPen  False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../fake_clean2.csv\",header=None)\n",
    "train.columns = [\"title1\",\"title2\",\"label\"]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_platform.txt\") as f:\n",
    "    news_platform = f.readlines()\n",
    "news_del = \"|\".join([re.sub(\"\\W+\",\"\",new) for new in news_platform])\n",
    "with open(\"other_platform.txt\") as f:\n",
    "    other_platform = f.readlines()\n",
    "other_del = \"|\".join([re.sub(\"\\W+\",\"\",new) for new in other_platform])\n",
    "_del = news_del+\"|\"+other_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>看看香港黃之鋒的惡行惡狀欺負老人家算什麼好漢暴徒來台卻變成英雄貴賓把台灣道德價值摧毀殆盡綠色...</td>\n",
       "      <td>錯誤網傳影片看看香港黃之鋒的惡行惡狀充分沒有教養的港獨暴力份子欺負老人家算什麼好漢</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3燒烤時戴隱形眼鏡遇高溫會使隱形眼鏡熔化而致盲</td>\n",
       "      <td>錯誤網傳在烤肉請將你的隱形眼鏡拿掉一個21歲男生生火時過熱的溫度熔化隱形眼鏡</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剛出爐的麵包傷胃</td>\n",
       "      <td>謠言終結站剛出爐的麵包會致癌專家斥無稽生活</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>昨晚香港一名參與示威女子被同伴擊中眼睛可能致視力嚴重受損他們竟還反污衊香港警方</td>\n",
       "      <td>早報中國央視報導香港女子右眼受傷引用假照片新聞信源無可稽考</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你還在吃龍眼嗎看完後震驚了一定要告訴家人啊分享</td>\n",
       "      <td>假養生近視從800度降到200度謠言讓醫生表示效果有限不要過量了</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title1  \\\n",
       "0  看看香港黃之鋒的惡行惡狀欺負老人家算什麼好漢暴徒來台卻變成英雄貴賓把台灣道德價值摧毀殆盡綠色...   \n",
       "1                            3燒烤時戴隱形眼鏡遇高溫會使隱形眼鏡熔化而致盲   \n",
       "2                                           剛出爐的麵包傷胃   \n",
       "3            昨晚香港一名參與示威女子被同伴擊中眼睛可能致視力嚴重受損他們竟還反污衊香港警方   \n",
       "4                            你還在吃龍眼嗎看完後震驚了一定要告訴家人啊分享   \n",
       "\n",
       "                                      title2  label  \n",
       "0  錯誤網傳影片看看香港黃之鋒的惡行惡狀充分沒有教養的港獨暴力份子欺負老人家算什麼好漢  False  \n",
       "1     錯誤網傳在烤肉請將你的隱形眼鏡拿掉一個21歲男生生火時過熱的溫度熔化隱形眼鏡  False  \n",
       "2                      謠言終結站剛出爐的麵包會致癌專家斥無稽生活  False  \n",
       "3              早報中國央視報導香港女子右眼受傷引用假照片新聞信源無可稽考  False  \n",
       "4           假養生近視從800度降到200度謠言讓醫生表示效果有限不要過量了  False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"title2\"] = train[\"title2\"].apply(lambda x:re.sub(\"\\W+\",\"\",x)).apply(lambda x:re.sub(_del,\"\",x))\n",
    "train[\"title1\"] = train[\"title1\"].apply(lambda x:re.sub(\"\\W+\",\"\",x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenized 建立語料庫 goV+Kaggle+ettoday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import re\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import jieba\n",
    "from langconv import *\n",
    "jieba.set_dictionary(\"dict.txt.big.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['太陽能發電比燃煤發電便宜 大陸宣布做到了 | 聯合新聞網', '人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！', '新華社於2018/12/29發布新聞稿，中國新聞無從查證起。', '太陽能發電比燃煤發電便宜 大陸宣布做到了 | 聯合新聞網', '人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！']\n"
     ]
    }
   ],
   "source": [
    "with open(\"all_corpus.txt\",\"r\",encoding=\"utf8\") as fr:\n",
    "    all_ls =[f.strip() for f in  fr.readlines()]\n",
    "print(all_ls[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    r = '\\W+'\n",
    "    sentence = Converter('zh-hant').convert(sentence)\n",
    "    words = re.sub(r,\"\",sentence)\n",
    "    wordarr = [word for word in jieba.cut(words)]\n",
    "    return \" \".join(wordarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\program\\semester_final\\clean_data\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\PC-Henry\\AppData\\Local\\Temp\\jieba.u6179e01b4c6492e8310058a8757f6034.cache\n",
      "Loading model cost 1.228 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            0\n",
      "0         太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了 聯合 新聞網\n",
      "1  人家 在 做 我們 在 譙 太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了\n",
      "2          新華社 於 20181229 發布 新聞稿 中國 新聞 無從查證 起\n",
      "3         太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了 聯合 新聞網\n",
      "4  人家 在 做 我們 在 譙 太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.DataFrame([clean_sentence(ls) for ls in all_ls])\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718662\n"
     ]
    }
   ],
   "source": [
    "K_train =  pd.read_csv(\"../train_p.csv\")\n",
    "corpus_x1 = K_train.title1_tokenized\n",
    "corpus_x2 = K_train.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "    corpus,corpus_x1, corpus_x2])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"semester\"]\n",
    "mycol = mydb[\"ettoday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        0\n",
      "0   快訊 高嘉瑜 晚間 好奇 找 阿北 求證 柯 P 終於 一句 話 解答 了\n",
      "1  美眉 的 夏天 最 HOT 名單 出爐 她 激戰 搶下 冠軍 抱 千元 大獎\n",
      "2       搭 西寧 軍艦 南偵 22 名 教師 登 太平島 深化 主權 意識\n",
      "3       桃園 區 慶 讚 中 元 水燈 排車 繞境 傳承 百年 特色 慶典\n",
      "4       鳳飛飛 故事 展 電影展 等 紀念活動 緬懷 永遠 的 國民 天后\n",
      "(817988, 1)\n"
     ]
    }
   ],
   "source": [
    "ettoday_t = []\n",
    "ettoday_c = []\n",
    "for i in mycol.find({},{\"title\":1,\"content\":1}):\n",
    "    ettoday_t.append(i[\"title\"].strip())\n",
    "    ettoday_c.append(i[\"content\"].strip())\n",
    "ettoday_title = pd.DataFrame([clean_sentence(ls) for ls in ettoday_t])\n",
    "ettoday_content = pd.DataFrame([clean_sentence(ls) for ls in ettoday_c])\n",
    "print(ettoday_title[:5])\n",
    "corpus = pd.concat([\n",
    "    corpus,ettoday_title, ettoday_content])\n",
    "print(corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817983, 1)\n",
      "[['太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了 聯合 新聞網'], ['人家 在 做 我們 在 譙 太陽能 發電 比 燃煤 發電 便宜 大陸 宣佈 做到 了']]\n"
     ]
    }
   ],
   "source": [
    "# 去除數字且降維(需要一維)\n",
    "corpus = corpus[corpus[0].str.isnumeric() == False]\n",
    "print(corpus.shape)\n",
    "corpus_a = corpus.values.tolist()\n",
    "print(corpus_a[:2])\n",
    "corpus_a = [i[0] for i in corpus_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(corpus_a)\n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone1zone3\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word.get(400000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 應該將所有資料集整合統一成一個 pickle\n",
    "with open(\"tokenized.pickle\",\"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
