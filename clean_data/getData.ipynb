{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://cofacts.g0v.tw/reply/A89kv2wBqwaEkHKwLTTZ\")\n",
    "html = etree.HTML(response.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['真的假的', '半夜闻到煤气味，开灯瞬间导致爆炸_李玉玲']\n"
     ]
    }
   ],
   "source": [
    "print(html.xpath(\"//h1/text()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "['德國聯邦議院電子連署平臺自 2005 年啟用以來，已進行多次修正及更多功能開發，請願頁面包含請願內容、討論區論壇、案號、請願書 PDF 檔下載、請願起始日、請願狀態、投票結果及回應\\n理由、連署數量（含線上及紙本）及時間進程（以折線圖呈現），同時也導入社群分享（Twitter、Google+及 Facebook）不同於亞洲國家的請願或連署機制，多以附議或連署人數做為權責機關是否出面回應的關鍵門檻，優先處理當下民意所關注的議題，歐美國家多以請願或提議內容為導向，沒有人數限制的情況下，每一位公民所提出的訴求都能被平等看待，權責機關亦可依據議題的重要性、急迫性及可行性來選擇與回覆。德國採綜合式機制，保留上述兩者之優點，即便審查的過程需耗費較多行政成本，卻能夠提供更多機關回應的機會，從多方面了解民眾對於公共議題的想法與建議。', '不同民調機構，甚至施作時間點及地方特性，首長滿意度起起伏伏很正常，只要認真面對市民，好好做事，對地方政府首長們，不用因民調成績就給予過多苛責。', '美國國務院東亞和太平洋事務局副助理國務卿費德瑋（Jonathan Fritz）說，美國非常擔憂外國對手企圖干預選舉，且北京肯定有偏愛的候選人，美台將會合作防止外界干預台灣選舉。', '政府收購的文旦格外品有 4000 公噸，去掉可以做果醬果醋果茶蜜餞或萃取精油做清潔劑的之後之，剩下的會拿去做堆肥。\\n\\n國民黨團所揭露的正是堆肥場，上頭有 1000 公噸文旦，僅佔全台文旦總產量 1%，差不多就是格外品處置的比例。\\n', '3D技術即是利用雙眼視差的原理，透過各種光學技術，讓左眼及右眼分別接收到不同的視角，使觀看者對畫面產生立體視覺。早期是以佩戴眼鏡的方式，例如紅藍雙色的濾色眼鏡、偏光式眼鏡。透過眼鏡的濾光，讓雙眼分別看到不同的畫面，產生立體視覺的效果。  里約大冒險的影片並沒有使用到3D裸視的做法。', '明白他為什麼“一定要選總統”的部分含有無法查證的陰謀論，而且外交部對此表示，目前沙國主政機關未確認此訊息，若沙國有意調降對我國人簽證費，外交部樂觀其成。', '美玉姨係使用Cofacts 真的假的 查核資料的機器人，Cofacts 的資料庫與 API 裡的資料以 CC0 貢獻至公共領域，任何人都可以接取。 背後沒有監控系統。', '1. 標題中的「家庭教育刪夫妻」讓人覺得「夫妻」兩個字在家庭教育中完全消失，但其實是行政命令的文字變更而已。\\n\\n現行家庭教育法施行細則中，家庭教育包括親職教育、性別教育、婚姻教育、倫理教育、多元文化教育等 8 種教育。其中「婚姻教育」定義原為「指增進夫妻關係之教育活動」，教育部因應少子化、高齡化時代，加上同婚專法通過，在修正草案則改為「指增進婚前及婚後關係經營之教育活動及服務」，並新增情緒教育、人口教育。\\n\\n2. 關於「老師抱頭在燒」，也有老師提出建議。《蘋果新聞網》訪問到資深國中小教師表示，會利用MV、廣告影片及繪本、新聞事件等當課堂教材，如蔡依林MV《不一樣又怎樣》、男男伴侶喜餅廣告，專法上路可藉此說明為何同志需婚姻保障及提供甚麼權利義務，讓學生很快抓到重點。\\n\\n高雄市立路竹高中國中部輔導老師楊嘉宏表示，要談同性婚姻合法化，會運用公民課從傳統兩性家庭談起，先談小家庭、中家庭概念，以此延伸到過往常被汙名化的單親家庭、隔代教養家庭到台灣越來越多的同志家庭，再告訴學生同婚合法化後，過去沒有保障的法律基礎已經受到認可。', '台灣人愛吃保健食品遠近馳名，根據尼爾森統計，台灣使用維生素與保健食品比例，與美國並列全球第4，超過四分之一的民眾每天都吃維生素與保健食品。保健食品只是營養補充劑，身體有需要時才補充。目前衛生署藥物食品管理局正積極收集各類保健食品成分與藥物間交互作用，提供民眾參考，以免民眾花大錢買保健食品，反而危及生命。\\n\\n養生風氣日盛，許多民眾對保健食品的觀念是多吃無害，而食藥署在過去一年曾接獲因過量食用保健食品，而導致身體出現不適的通報案件達30件，其中以國人最常食用的維生素項目最多，醫師呼籲，保健食品含量已不同於一般食物，民眾在食用前一定要遵照指示，詢問專業醫師人員，否則多吃恐傷身。', '真的假的cofacts可以是好人也可以是壞人，因為只要你願意加入闢謠，你就可以是真的假的cofacts千萬個聲音中的一個。請問，你要當好人，還是壞人？你怎麼確定自己一定是好人還是壞人？', '這是假的!\\n一般免費貼圖下載網址為\"https://line.me/S/Sticker/......\"開頭, 此網址不正確, 請留意假如用戶在過程中提供自己的LINE ID，有可能成為部分不肖業者發送商業廣告或色情等內容的對象，也有可能待帳號累積大量好友數後，就將帳號轉賣，或甚至轉成其他違法用途。', '陳志傑為全民拔蔡總部成員與退警總會執行秘書，曾參與多次抗議行動，在2018-08-11即曾指控國安局指使市警局\\n此影片可能同樣是因同樣問題被制止，至於影片中人物是否為國安局無從得知，國安局主要工作為情蒐與特勤規劃，除非有重要人士在場，否則確實沒有到場的必要', '此仍為發展中事件。', '查無文章來源，應為個人意見', '蔡英文總統 出席「108年下半年陸海空軍將官晉任布達暨授階典禮」，肯定晉任將官傑出表現，並重申將給予國軍最堅定的支持，期盼國軍時時以捍衛國家安全為己任，不負國人託付及期待。', '你應該做的，只是放過你自己。關於父母，你必須先好好照顧你自己，才能量力而為，做你能做的事情。 對於所有的人際關係，你唯一能做的，也就是量力而為而已。', '「外省人」內部也有各省的語言與文化習俗差異，第一代遷台移民之間尚有鮮明的原鄉省份意識，要到在台灣成長第二代在1970年代以後，因為蔣經國為了安撫台灣人而刻意拔擢台籍菁英，才逐漸發展出「本省籍成為優秀籍貫」的「外省人弱勢群體意識」之原型，並在1980年代討論「國會全面改選」的浪潮中，因為擔心外省籍青年未來在中央民意代表選舉中，無法產生符合人口比例的代表，而逐漸發展出「外省人弱勢論」的政治運動論述（王甫昌2016）。', '在2019年5月17日立法院通過同性婚姻專法後，很快的，結婚就不再是特定人士的特權 。', '身體發熱、流汗、鼻涕濃稠、黃痰、容易口渴、喉嚨紅腫疼痛、鼻竇發炎等等，都屬於風熱感冒的特徵。 \\n\\n 其實台灣是南方小島，如果不是太勞累又吹風，感冒大多屬於熱性感冒，容易發炎發熱，這時如果狂喝薑湯，反而會因為太燥，讓發炎情況加劇，讓感冒變得更不容易好。 ', '身為台灣人的我們要知道，「不改變現狀」其實是一個不斷在默默改變的一個狀態，靠著來自外部的仁慈而偏安一隅，不可能是一個終極的狀態。', '馬凱碩說，19世紀世界由英國控制，20世紀由美國掌控，但這不代表21世紀就是中國的世紀。', '國安局表示，在北檢起訴後，國安局即刻與初始行政調查的人員自清內容進行比對，對於初期未行自清、現遭偵查涉案者，將給予檢討調回國軍編制與行政懲處。', '針對這個訊息裡提到的關鍵數據，經濟日報有不同的解讀。\\n\\n1️⃣ 鴻海的總負債已經高達一兆六千四百二十三億元 ❓\\n💁 是的。鴻海負債總額看起來很高，主要是因為公司規模大，所以相對的數字也高，但若仔細看「負債比例」卻是同類型下游電子代工產業中的優等生。我們從公開資訊觀測站中調出資料來看，以今年第一季與第二季為例，鴻海在下游電子代工產業中負債占資產比例都是最低的。\\n\\n2️⃣鴻海的總負債高居全台灣電子業第一名，流動負債金額是台灣上市公司之冠❓\\n📝檢視財務不能只看單一指標，不能單看負債！財務報表中有「資產負債表」就是要同時看資產與負債這兩項數據！如果以償債能力來評論，最基本的就是看流動資產以及流動負債的對比，也就是流動比例，流動比例越高代表流動資產越高、償債能力越好，第一季鴻海流動比例174%，更遠遠勝於其他電子同業。數字會說話，以證交所提供的資料來看，可以很明顯看出差異。\\n\\n3️⃣從明年開始，鴻海每年都要償還兩百餘億元的普通公司債❓\\n👌是的，但其實對公司財務壓力不大。攤開財報來算，鴻海第一季淨現金2495億以及第二季淨現金2398億，這個數字是把現金定存、理財商品全部變現，並償還銀行借款及公司債之後，還剩下來的現金，還有兩千多億。對於金融業來說，絕對不會希望公司把這些借款以及公司債還掉，鴻海的信用評等A-穩定，對金融業來說，是非常好的貸放目標。對股東來說，他們也會希望公司能利用低利率的有利條件來作靈活的營運，以創造更多的股東權益。\\n\\n', '此為讀者投書，並非客觀事實。', '此文出處應為風傳媒整理報導，事件為仍在進行中事件', '德國聯邦議院電子連署平臺自 2005 年啟用以來，已進行多次修正及更多功能開發，請願頁面包含請願內容、討論區論壇、案號、請願書 PDF 檔下載、請願起始日、請願狀態、投票結果及回應\\n理由、連署數量（含線上及紙本）及時間進程（以折線圖呈現），同時也導入社群分享（Twitter、Google+及 Facebook）不同於亞洲國家的請願或連署機制，多以附議或連署人數做為權責機關是否出面回應的關鍵門檻，優先處理當下民意所關注的議題，歐美國家多以請願或提議內容為導向，沒有人數限制的情況下，每一位公民所提出的訴求都能被平等看待，權責機關亦可依據議題的重要性、急迫性及可行性來選擇與回覆。德國採綜合式機制，保留上述兩者之優點，即便審查的過程需耗費較多行政成本，卻能夠提供更多機關回應的機會，從多方面了解民眾對於公共議題的想法與建議。', '不同民調機構，甚至施作時間點及地方特性，首長滿意度起起伏伏很正常，只要認真面對市民，好好做事，對地方政府首長們，不用因民調成績就給予過多苛責。', '美國國務院東亞和太平洋事務局副助理國務卿費德瑋（Jonathan Fritz）說，美國非常擔憂外國對手企圖干預選舉，且北京肯定有偏愛的候選人，美台將會合作防止外界干預台灣選舉。', '政府收購的文旦格外品有 4000 公噸，去掉可以做果醬果醋果茶蜜餞或萃取精油做清潔劑的之後之，剩下的會拿去做堆肥。\\n\\n國民黨團所揭露的正是堆肥場，上頭有 1000 公噸文旦，僅佔全台文旦總產量 1%，差不多就是格外品處置的比例。\\n', '3D技術即是利用雙眼視差的原理，透過各種光學技術，讓左眼及右眼分別接收到不同的視角，使觀看者對畫面產生立體視覺。早期是以佩戴眼鏡的方式，例如紅藍雙色的濾色眼鏡、偏光式眼鏡。透過眼鏡的濾光，讓雙眼分別看到不同的畫面，產生立體視覺的效果。  里約大冒險的影片並沒有使用到3D裸視的做法。']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://cofacts.g0v.tw/replies?after=&before=&filter=OPINIONATED\")\n",
    "html = etree.HTML(response.content)\n",
    "ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]')\n",
    "next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "total_number = html.xpath('//p[@class=\"jsx-3107014753\"]/text()')[0]\n",
    "all_ls = []\n",
    "while(len(all_ls)<int(total_number)):\n",
    "    for l in ls:\n",
    "        all_ls.append(l.xpath(\".//div[@class='jsx-3552945388 jsx-3720007368 item-text']/text()\")[1])\n",
    "        if(len(all_ls)%50 == 0 ):\n",
    "            print(len(all_ls))\n",
    "    response = requests.get(\"https://cofacts.g0v.tw/replies?\"+next_link)\n",
    "    html = etree.HTML(response.content)\n",
    "    ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]')\n",
    "    next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import threading\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value(title):\n",
    "    pipe = [\"維基\",\"Server Error\",\"Access denied\",\"Yahoo is now\",\"Page Not Found\",\"痞客邦\",\"百度\",\"404\",\"Security\"]\n",
    "    if any(x in title for x in pipe):\n",
    "        return True\n",
    "    elif (re.search(\"[^a-zA-Z0-9\\b]+\",title)):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news(l,all_ls):\n",
    "    newslink = hostName+ l\n",
    "    response = requests.get(newslink);\n",
    "    html = etree.HTML(response.content)\n",
    "    origin_news = \"\".join(html.xpath(\"//div[@class='jsx-1919529045 message']/text()\"))\n",
    "    if(len(origin_news)>70 or len(origin_news)<3 or check_value(origin_news)) :\n",
    "        return 0;\n",
    "    new_titles = html.xpath(\"//section[@class='jsx-588669885 links']//article[@class='jsx-1682249194 link']/h1/@title\")\n",
    "    reason = \"\".join(html.xpath(\"//section[@class='jsx-3677418999 jsx-423907629 section']//div/text()\"))\n",
    "    for i in new_titles:\n",
    "        if(check_value(i)):\n",
    "            return 0\n",
    "        dictionary = {}\n",
    "        dictionary[\"link\"] = newslink\n",
    "        dictionary[\"news1\"] = origin_news\n",
    "        dictionary[\"news2\"] = i\n",
    "        dictionary[\"fact\"] = False\n",
    "        dictionary[\"reason\"] = reason\n",
    "        all_ls.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "hostName = \"https://cofacts.g0v.tw\"\n",
    "response = requests.get(\"https://cofacts.g0v.tw/replies?after=&before=&filter=RUMOR\")\n",
    "html = etree.HTML(response.content)\n",
    "newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "total_number = html.xpath('//p[@class=\"jsx-3107014753\"]/text()')[0]\n",
    "all_ls = []\n",
    "count = 0\n",
    "while(True):\n",
    "    threads = []\n",
    "    for l in newslink_ls:\n",
    "        count += 1\n",
    "        threads.append(threading.Thread(target=clean_news,args=(l,all_ls)))\n",
    "        threads[-1].start()\n",
    "        if(count%100 == 0  and count != 0):\n",
    "            print(count)\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    response = requests.get(\"https://cofacts.g0v.tw/replies?\"+next_link)\n",
    "    html = etree.HTML(response.content)\n",
    "    try:\n",
    "        newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "        next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "    except:\n",
    "        break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = all_ls[0].keys()\n",
    "with open('fake.csv', 'w',encoding=\"utf8\") as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data(l,all_ls):\n",
    "    newslink = hostName+ l\n",
    "    response = requests.get(newslink);\n",
    "    html = etree.HTML(response.content)\n",
    "    origin_news = \"\".join(html.xpath(\"//div[@class='jsx-1919529045 message']/text()\"))\n",
    "    if(len(origin_news)<3 or check_value(origin_news)) :\n",
    "        return 0;\n",
    "    new_titles = html.xpath(\"//section[@class='jsx-588669885 links']//article[@class='jsx-1682249194 link']/h1/@title\")\n",
    "    reason = \"\".join(html.xpath(\"//section[@class='jsx-3677418999 jsx-423907629 section']//div/text()\"))\n",
    "    for i in new_titles:\n",
    "        if(check_value(i)):\n",
    "            return 0\n",
    "        all_ls.append(i)\n",
    "    all_ls.append(origin_news)\n",
    "    all_ls.append(reason)\n",
    "#         dictionary = {}\n",
    "#         dictionary[\"link\"] = newslink\n",
    "#         dictionary[\"news1\"] = origin_news\n",
    "#         dictionary[\"news2\"] = i\n",
    "#         dictionary[\"fact\"] = False\n",
    "#         dictionary[\"reason\"] = reason\n",
    "#         all_ls.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "hostName = \"https://cofacts.g0v.tw\"\n",
    "response = requests.get(\"https://cofacts.g0v.tw/replies\")\n",
    "html = etree.HTML(response.content)\n",
    "newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "total_number = html.xpath('//p[@class=\"jsx-3107014753\"]/text()')[0]\n",
    "all_ls = []\n",
    "count = 0\n",
    "while(True):\n",
    "    threads = []\n",
    "    for l in newslink_ls:\n",
    "        count += 1\n",
    "        threads.append(threading.Thread(target=clean_all_data,args=(l,all_ls)))\n",
    "        threads[-1].start()\n",
    "        if(count%100 == 0  and count != 0):\n",
    "            print(count)\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    if(count>=int(total_number)):\n",
    "        break\n",
    "\n",
    "    response = requests.get(\"https://cofacts.g0v.tw/replies?\"+next_link)\n",
    "    html = etree.HTML(response.content)\n",
    "    try:\n",
    "        newslink_ls = html.xpath('//a[@class=\"jsx-3552945388 jsx-3720007368 item\"]/@href')\n",
    "        next_link = html.xpath('//a[@class=\"jsx-684928770\"]/@href')[1]\n",
    "    except:\n",
    "        break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_corpus.txt\",\"w\",encoding=\"utf8\") as fw:\n",
    "    for i in all_ls:\n",
    "        fw.writelines(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['太陽能發電比燃煤發電便宜 大陸宣布做到了 | 聯合新聞網\\n',\n",
       " '人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！\\n',\n",
       " '新華社於2018/12/29發布新聞稿，中國新聞無從查證起。\\n',\n",
       " '太陽能發電比燃煤發電便宜 大陸宣布做到了 | 聯合新聞網\\n',\n",
       " '人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！\\n',\n",
       " '技術逐年進展，也讓太陽光電和陸域風電裝設成本下降、容量因數（Capacity Factor，可理解為全年發電比例）上升，使全生命週期的均化發電成本大跌。\\n',\n",
       " '楊安澤，雲林人之子，美國台裔商人，也是第一個民主黨爭取2020年美國總統大選的提名人，也是第一位宣布競選的台裔美國人。楊安澤的父親《楊界雄》是台灣面板教父，雲林元長田洋村人，畢業於元長仁愛國小，是《虎中傑出校友》，兩岸及美國科技界尖端人才，都有《楊界雄教授》的學生。（感謝~陳榮俊特派的分享）台大物理系畢業與服役後即申請到美國印地安那州聖母大學物理系的獎學金，完成碩士學位後又以優異成績申請到柏克萊加州大學的獎學金，在該校的物理系完成了博士學位。楊界雄博士，是柏克萊加大的物理博士，獲得美國69項專利。曾任職於貝爾（Bell）實驗室，奇異(GE)研發中心和IBM華生（Watson）研究所，如此超優異的學經歷回國後便服務於瀚宇彩晶公司擔任研究中心副總，並於其領導下成功突破多年來液晶封裝專利的限制，爲全世界的光電顯示器業界投下一顆震撼彈。並於2001年榮獲國際資訊顯示學會院士(SID Fellow)。楊界雄教授，經常鼓勵年輕人說：「年輕人就是要有guts！」其優秀的公子~楊安澤，在人生旅途中，也就是《非常有guts的表現》。我相信~台裔 楊安澤，將會全力以赴來落實所參選之路，宣布参選要相當勇氣、也要相當實力，認真的男人永遠最帥！不管，結果是如何，雲林人第二代，台裔 楊安澤，肯定是最棒，最傑出的！楊安澤，是雲林之光！更是台灣之耀！好友，若是您有美國友人，請您轉達請支持~台裔美國人 楊安澤 2020年美國民主黨提名參選人。\\n',\n",
       " '雲林元長鄉沒有田洋村好嗎?他是潭西村人\\n',\n",
       " '廣東省原副省長萬慶良被宣判判處無期徒刑！查封519億的人民幣及財產全部歸回國庫。判決書由萬慶良自己讀！\\n',\n",
       " '中國南寧市中級人民法院於2016/9/30宣判，但中國新聞無法查證。\\n',\n",
       " '万庆良 - 维基百科，自由的百科全书\\n',\n",
       " '廣東省原副省長萬慶良被宣判判處無期徒刑！查封519億的人民幣及財產全部歸回國庫。判決書由萬慶良自己讀！\\n',\n",
       " '南寧市中級人民法院公開宣判，萬慶良犯受賄罪，判處無期徒刑，剝奪政治權利終身，並處沒收個人全部財產；對萬慶良受賄所得財物予以追繳，上繳國庫。\\n',\n",
       " '1992年洛杉磯暴動 - 维基百科，自由的百科全书\\n',\n",
       " '香港的暴動可以媲美當時在洛杉磯的暴力動亂。1992年洛杉磯暴動（又被稱作1992年洛杉磯內亂， 英語：1992 Los Angeles Civil Unrest/1992 Los Angeles riots），主要指1992年於美國加州南部大城洛杉磯市爆發的一系列動亂，導火線為該年4月29日當地陪審團宣判四名被控「使用過當武力」的警察無罪釋放，導致上千名對此判決不滿的非裔與拉丁裔上街抗議，最終引發一連串暴動，波及包括亞裔（特別是居於城中的韓裔）在內的各社群。系爭判決為三名歐洲裔白人警察和一名拉丁裔白人警察毆打涉及交通違規事件的一名黑人羅德尼·金。根據事後統計，整起內亂造成各方約10億美元的財產損失，並有約53人於暴動中死亡，數千人受輕重傷，震驚全球。\\n',\n",
       " '香港反送中(反對逃犯條例修訂草案運動)，是港人爭取人權和法治，大規模和平集會、遊行的運動；洛杉磯暴動為種族不平等待遇和所引發的衝突和暴動。天差地遠。\\n',\n",
       " '蔡英文準備發老人津貼\\n',\n",
       " '目前沒有這項政策的發送，「準備」屬於個人臆測，並不是事實。\\n',\n",
       " '被高雄政府「找麻煩」？大港開唱改口：說「停辦」太沉重，明年先休息 - The News Lens 關鍵評論網\\n',\n",
       " '「大港開唱」自四年前起，是由中央政府文化部主導補助！今年為了「卡韓」，中央故意不補助！韓國瑜決定，既使高雄欠債三千三百億，還是願意編列預算「繼續唱！」，但主辦人屎袋「林場佐」，過去十年辦活動，不僅獲得五千萬元的高額政府補助，還出售高價入場門票，獲得近四億元的收入，政府補助的納稅人血汗錢，連招標、預算編列、議會審查等通通都可以省略！結案報告更是年年照抄！今年看到新上任的高雄市政府，不能走後門，需要依法來申請經費，於是就聯手蔡英蚊總統，乾脆不申請補助、直接栽贓高雄市政府停辦！一個字「賤！」#請踴躍幫忙分享轉發#唯一支持韓國瑜\\n',\n",
       " '林昶佐已於2016年當選立委時退出主辦團隊，本案目前尚無進一步收入與財報公開資料，所述數字並無根據。\\n',\n",
       " '懶得鳥你 官方表情貼 – LINE表情貼 | LINE STORE\\n',\n",
       " '[懶得鳥你 官方表情貼]\\n',\n",
       " '這是商業活動廣告 \\n',\n",
       " '被高雄政府「找麻煩」？大港開唱改口：說「停辦」太沉重，明年先休息 - The News Lens 關鍵評論網\\n',\n",
       " '陳菊自豪辦了10幾年的演唱會這就是用高雄市政府的公帑用來辦的大港開唱，您聽聽他們說了什麼，如果明年大選不翻轉台灣，那我們只能一起沉淪了！用中華民國納稅人的錢搞特定補貼、玩政治、騙選票、分裂族群、滿口粗話，低賤的合唱團，納稅人，你不生氣嗎？2020勢必反轉台灣，下架民進黨，復興中華民國！\\n',\n",
       " '大港開唱為使用中央政府文化局經費，補助部份的演唱會，且自2010年起舉辦尚未達10年。目前無明確證據為特定補助。\\n',\n",
       " '要老師分享招生貼文 計入年終點數 工會批：把老師當網紅 - 生活 - 自由時報電子報\\n',\n",
       " '科大要求教師當網紅FB分享學校文 私校工會批：不尊重教師尊嚴 | 立報傳媒\\n',\n",
       " '要老師分享招生貼文 計入年終點數 工會批：把老師當網紅 - 生活 - 自由時報電子報 \\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langconv import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"dict.txt.big.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = []\n",
    "with open(\"all_corpus.txt\",encoding=\"utf8\") as file:\n",
    "    count = 0;\n",
    "    for li in file:\n",
    "        all_.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HanziConv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3371f9de029f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHanziConv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoTraditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHanziConv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoTraditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HanziConv' is not defined"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "with open(\"fake.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    next(rows)\n",
    "    for row in rows:\n",
    "        if(len(row)>0):\n",
    "            arr.append(HanziConv.toTraditional(row[1]))\n",
    "            arr.append(HanziConv.toTraditional(row[2]))\n",
    "\n",
    "with open(\"unfake.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    next(rows)\n",
    "    for row in rows:\n",
    "        if(len(row)>0):\n",
    "            arr.append(HanziConv.toTraditional(row[1]))\n",
    "            arr.append(HanziConv.toTraditional(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple2tradition(line):\n",
    "    line = Converter('zh-hant').convert(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\program\\semester_final\\clean_data\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\PC-Henry\\AppData\\Local\\Temp\\jieba.u6179e01b4c6492e8310058a8757f6034.cache\n",
      "Loading model cost 1.346 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "fileTrainSeg=[]\n",
    "SegTwoDimen = []\n",
    "r = '\\W+'\n",
    "for i in range(len(all_)):\n",
    "    all_[i] = simple2tradition(all_[i])\n",
    "    words = re.sub(r,\"\",all_[i])\n",
    "    wordarr = [word for word in jieba.cut(words)]\n",
    "    SegTwoDimen.append(wordarr);\n",
    "    fileTrainSeg.append([' '.join(wordarr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegTwoDimen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成!!\n"
     ]
    }
   ],
   "source": [
    "fileSegWordDonePath ='corpusSegDone.txt'\n",
    "with open(fileSegWordDonePath,'wb') as fW:\n",
    "    for i in range(len(fileTrainSeg)):\n",
    "        fW.write(fileTrainSeg[i][0].encode('utf-8'))\n",
    "        fW.write(\"\\n\".encode(\"utf-8\"))\n",
    "    print(\"完成!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models, similarities\n",
    "from langconv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fileTrainSeg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-93a990a575c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mPrintListChinese\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileTrainSeg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fileTrainSeg' is not defined"
     ]
    }
   ],
   "source": [
    "def PrintListChinese(list):\n",
    "    for i in range(len(list)):\n",
    "        print(list[i])\n",
    "        \n",
    "PrintListChinese(fileTrainSeg[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(\"corpusSegDone.txt\")\n",
    "model = word2vec.Word2Vec(sentences, window=5, min_count=5, size=200, sg=0, workers=32, hs=1, iter=5)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "corpora_dict_path = 'corpora_dict.dict'\n",
    "dictionary = corpora.Dictionary(SegTwoDimen)\n",
    "dictionary.save(corpora_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dict_path = 'corpora_dict.dict'\n",
    "tfidf_model_path = \"./tfidf.model\"\n",
    "dictionary = corpora.Dictionary(SegTwoDimen)\n",
    "dictionary.save(corpora_dict_path)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in SegTwoDimen]\n",
    "corpora.MmCorpus.serialize('./corpus.mm', corpus) \n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf.save(tfidf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dict_path = 'corpora_dict.dict'\n",
    "tfidf_model_path = \"./tfidf.model\"\n",
    "corpus_path = './corpus.mm'\n",
    "corpus = corpora.MmCorpus(corpus_path)\n",
    "tfidf = models.TfidfModel.load(tfidf_model_path)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "dictionary = corpora.Dictionary.load(corpora_dict_path)\n",
    "index = similarities.Similarity(\"./tempName\",corpus_tfidf,len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dict_path = 'corpora_dict.dict'\n",
    "tfidf_model_path = \"./tfidf.model\"\n",
    "corpus_path = './corpus.mm'\n",
    "corpus = corpora.MmCorpus(corpus_path)\n",
    "tfidf = models.TfidfModel.load(tfidf_model_path)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "dictionary = corpora.Dictionary.load(corpora_dict_path)\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary)\n",
    "index = similarities.Similarity(\"./tempName\",lsi[corpus],len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "def clean_sentence(sentence):\n",
    "    r = '[’!\"#$%&\\'()*+,./:;<=>?@，。?★、…【】\\n《》\\s？“”‘’�！[\\\\]^_`{|}~「」:]+'\n",
    "    sentence = simple2tradition(sentence)\n",
    "    words = re.sub(r,\"\",sentence)\n",
    "    wordarr = [word for word in jieba.cut(words)]\n",
    "    return \" \".join(wordarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC-Henry\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.711 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_doc = [\"太陽能發電比燃煤發電便宜大陸宣佈做到了聯合新聞網\",\"人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！\"]\n",
    "test_doc = [clean_sentence(text) for text in test_doc]\n",
    "new_vecs = [dictionary.doc2bow(test.split()) for test in test_doc]\n",
    "new_vec_tfidf_ls = [tfidf[new_vec] for new_vec in new_vecs] \n",
    "sims = [index[new_vec_tfidf] for new_vec_tfidf in new_vec_tfidf_ls]\n",
    "# sims = [sorted(list(zip(_id_ls,sim)),key= lambda tup:tup[1],reverse=True) for sim in sims]\n",
    "# [print(sentence_dict[sim[0][0]]) for sim in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "index = np.argmax(np.array(sims[0]))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.        , 0.8253497 , 0.        , ..., 0.        , 0.00233107,\n",
      "       0.        ], dtype=float32), array([0.8253497 , 1.        , 0.        , ..., 0.        , 0.00403139,\n",
      "       0.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensimCalculation(d1, d2):\n",
    "    text1 = [text for text in jieba.cut(d1)]\n",
    "    texts = [[text for text in jieba.cut(d2)]]\n",
    "    print(text1,texts)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    feature_cnt = len(dictionary.token2id)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    new_vec = dictionary.doc2bow(text1)\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt)\n",
    "    print(tfidf[new_vec])\n",
    "    sim = index[tfidf[new_vec]]\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.78473645, 0.7787981 , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ], dtype=float32), array([0.63801837, 0.8332558 , 0.        , ..., 0.        , 0.00105189,\n",
      "       0.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['技術', '逐年', '進展', '，', '也', '讓', '太陽', '光電', '和', '陸域', '風電裝設', '成本', '下降'] [['人家', '在', '做', '、', '我們', '在', '譙', '！', '太陽能', '發電比', '燃煤', '發電', '便宜', ' ', '大陸', '宣布', '做到', '了', '！']]\n",
      "[]\n",
      "[0.]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(gensimCalculation(\"技術逐年進展，也讓太陽光電和陸域風電裝設成本下降\",\"人家在做、我們在譙！太陽能發電比燃煤發電便宜 大陸宣布做到了！\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.78473645, 0.7787981 , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langconv import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "with open(\"fake.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0):\n",
    "            continue\n",
    "        if()\n",
    "        arr.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar in arr:\n",
    "    ar[1] = Converter('zh-hant').convert(ar[1])\n",
    "    ar[2] = Converter('zh-hant').convert(ar[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake.csv\",\"w\",encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for ar in arr:\n",
    "        writer.writerow(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9c72c47c3e58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"unrelated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'zh-hant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"agreed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langconv import *\n",
    "import csv\n",
    "all_ = []\n",
    "with open(\"compete_train.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    next(rows,None)\n",
    "    for row in rows:\n",
    "        if(row[7]!=\"unrelated\"):\n",
    "            temp = [Converter('zh-hant').convert(sent).replace('\"',\"\").replace('\\xa0',\"\") for sent in row[3:5]]\n",
    "            if(row[7] == \"agreed\"):\n",
    "                temp.append(True)\n",
    "            else:\n",
    "                temp.append(False)\n",
    "            all_.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"compete_train.csv\",\"w\",encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for a in all_:\n",
    "        writer.writerow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "all_ = []\n",
    "with open(\"unfake_clean.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0 or row[0] == row[1] or abs(len(row[0])-len(row[1]))>15):\n",
    "            continue\n",
    "        all_.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unfake_clean.csv\",\"w\",encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1353\n"
     ]
    }
   ],
   "source": [
    "all_ = []\n",
    "want = \"家樂福60歲囉\"\n",
    "dont = \"\"\n",
    "with open(\"fake_clean2.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0):\n",
    "            continue\n",
    "        if(re.search(want,row[1])):\n",
    "            continue\n",
    "        all_.append(row)\n",
    "print(len(all_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake_clean2.csv\",\"w\",encoding=\"utf8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2777\n"
     ]
    }
   ],
   "source": [
    "all_ = []\n",
    "with open(\"unfake_clean.csv\",encoding=\"utf8\") as file:\n",
    "    rows = csv.reader(file)\n",
    "    for row in rows:\n",
    "        if(len(row) == 0):\n",
    "            continue\n",
    "        all_.append(row)\n",
    "print(len(all_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langconv import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                          title1_zh  \\\n",
       "0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "3   2     2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "4   9     6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                    title2_zh  \\\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2        GDP首超香港？深圳澄清：还差一点点……   \n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4  \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                           title2_en      label  \n",
       "0  Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "3  Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "4  It took 30 years of cooking oil to know that o...     agreed  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = pd.read_csv(\"train.csv\")\n",
    "datas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title1_zh                   title2_zh      label\n",
       "0      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "1  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "2  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……  unrelated\n",
       "3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  unrelated\n",
       "4               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     agreed"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = datas.loc[:,[\"title1_zh\",\"title2_zh\",\"label\"]]\n",
    "datas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simtotra(text):\n",
    "    return Converter('zh-hant').convert(text)\n",
    "def stringfy(text):\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017養老保險又新增兩項，農村老人人人可申領，你領到了嗎</td>\n",
       "      <td>警方闢謠“鳥巢大會每人領5萬” 仍有老人堅持進京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳統計局闢謠：只是差距在縮小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：還差一點點……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳統計局闢謠：還差611億</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"用大蒜鑒別地溝油的方法,怎麼鑒別地溝油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜輕鬆鑒別地溝油</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title1_zh                   title2_zh      label\n",
       "0      2017養老保險又新增兩項，農村老人人人可申領，你領到了嗎    警方闢謠“鳥巢大會每人領5萬” 仍有老人堅持進京  unrelated\n",
       "1  \"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港   深圳GDP首超香港？深圳統計局闢謠：只是差距在縮小  unrelated\n",
       "2  \"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港        GDP首超香港？深圳澄清：還差一點點……  unrelated\n",
       "3  \"你不來深圳，早晚你兒子也要來\"，不出10年深圳人均GDP將超香港  去年深圳GDP首超香港？深圳統計局闢謠：還差611億  unrelated\n",
       "4               \"用大蒜鑒別地溝油的方法,怎麼鑒別地溝油     吃了30年食用油才知道，一片大蒜輕鬆鑒別地溝油     agreed"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[\"title1_zh\"] = datas.loc[:,\"title1_zh\"].apply(stringfy).apply(simtotra)\n",
    "datas[\"title2_zh\"] = datas.loc[:,\"title2_zh\"].apply(stringfy).apply(simtotra)\n",
    "datas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langconv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simtotra(text):\n",
    "    return Converter('zh-hant').convert(text)\n",
    "def stringfy(text):\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title1_zh                    title2_zh\n",
       "0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？\n",
       "1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国\n",
       "2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思\n",
       "3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！\n",
       "4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test = test.loc[:,[\"title1_zh\",\"title2_zh\"]]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>薩拉赫人氣爆棚!埃及總統大選未參選獲百萬選票 現任總統壓力山大</td>\n",
       "      <td>闢謠！里昂官方否認費基爾加盟利物浦，難道是價格沒談攏？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>薩達姆被捕後告誡美國的一句話，發人深思</td>\n",
       "      <td>10大最讓美國人相信的荒誕謠言，如蜥蜴人掌控著美國</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>薩達姆此項計劃沒有此國破壞的話，美國還會對伊拉克發動戰爭嗎</td>\n",
       "      <td>薩達姆被捕後告誡美國的一句話，發人深思</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>薩達姆被捕後告誡美國的一句話，發人深思</td>\n",
       "      <td>被絞刑處死的薩達姆是替身？他的此男人舉動擊破替身謠言！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>薩達姆被捕後告誡美國的一句話，發人深思</td>\n",
       "      <td>中國川貝枇杷膏在美國受到熱捧？純屬謠言！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title1_zh                    title2_zh\n",
       "0  薩拉赫人氣爆棚!埃及總統大選未參選獲百萬選票 現任總統壓力山大  闢謠！里昂官方否認費基爾加盟利物浦，難道是價格沒談攏？\n",
       "1              薩達姆被捕後告誡美國的一句話，發人深思    10大最讓美國人相信的荒誕謠言，如蜥蜴人掌控著美國\n",
       "2    薩達姆此項計劃沒有此國破壞的話，美國還會對伊拉克發動戰爭嗎          薩達姆被捕後告誡美國的一句話，發人深思\n",
       "3              薩達姆被捕後告誡美國的一句話，發人深思  被絞刑處死的薩達姆是替身？他的此男人舉動擊破替身謠言！\n",
       "4              薩達姆被捕後告誡美國的一句話，發人深思         中國川貝枇杷膏在美國受到熱捧？純屬謠言！"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"title1_zh\"] = test[\"title1_zh\"].apply(stringfy).apply(simtotra)\n",
    "test[\"title2_zh\"] = test[\"title2_zh\"].apply(stringfy).apply(simtotra)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test[\"title2_zh\"] != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"../test_p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
